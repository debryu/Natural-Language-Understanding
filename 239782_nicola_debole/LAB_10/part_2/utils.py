# Add functions or classes used for data loading and preprocessing
import torch
import torch.nn as nn
import torch.utils.data as data
from collections import Counter
from main import device
import json
from pprint import pprint
from model import bert_tokenizer
from torch.utils.data import DataLoader
# Define the ID for the pad token
PAD_TOKEN = bert_tokenizer.pad_token_id

def load_data(path):
    '''
        args:
            input: path/to/data
        returns:
            output: json 
    '''
    dataset = []
    with open(path) as f:
        dataset = json.loads(f.read())
    return dataset

def init_weights(mat):
    for m in mat.modules():
        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:
            for name, param in m.named_parameters():
                if 'weight_ih' in name:
                    for idx in range(4):
                        mul = param.shape[0]//4
                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])
                elif 'weight_hh' in name:
                    for idx in range(4):
                        mul = param.shape[0]//4
                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])
                elif 'bias' in name:
                    param.data.fill_(0)
        else:
            if type(m) in [nn.Linear]:
                torch.nn.init.uniform_(m.weight, -0.01, 0.01)
                if m.bias != None:
                    m.bias.data.fill_(0.01)



def collate_fn(data):
    def merge(sequences):
        '''
        merge from batch * sent_len to batch * max_len 
        '''
        lengths = [len(seq) for seq in sequences]
        max_len = 1 if max(lengths)==0 else max(lengths)
        # Pad token is zero in our case
        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape 
        # batch_size X maximum length of a sequence
        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(PAD_TOKEN)
        for i, seq in enumerate(sequences):
            end = lengths[i]
            padded_seqs[i, :end] = seq # We copy each sequence into the matrix
        # print(padded_seqs)
        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph
        return padded_seqs, lengths
    # Sort data by seq lengths
    data.sort(key=lambda x: len(x['utterance']), reverse=True) 
    
    new_item = {}
    for key in data[0].keys():
        new_item[key] = [d[key] for d in data]
    # We just need one length for packed pad seq, since len(utt) == len(slots)
    src_utt, _ = merge(new_item['utterance'])
    y_slots, y_lengths = merge(new_item["slots"])
    intent = torch.LongTensor(new_item["intent"])
    
    src_utt = src_utt.to(device) # We load the Tensor on our seleceted device
    y_slots = y_slots.to(device)
    intent = intent.to(device)
    y_lengths = torch.LongTensor(y_lengths).to(device)
    
    new_item["utterances"] = src_utt
    new_item["intents"] = intent
    new_item["y_slots"] = y_slots
    new_item["slots_len"] = y_lengths
    return new_item

class IntentsAndSlots (data.Dataset):
    # Mandatory methods are __init__, __len__ and __getitem__
    def __init__(self, dataset, lang, unk='unk'):
        self.utterances = []
        self.intents = []
        self.slots = []
        self.unk = unk
        
        for x in dataset:
            self.utterances.append(x['utterance'])
            self.slots.append(x['slots'])
            self.intents.append(x['intent'])

        self.utt_ids = self.mapping_bert(self.utterances, lang.word2id)
        self.slot_ids = self.mapping_seq(self.slots, lang.slot2id)
        self.intent_ids = self.mapping_lab(self.intents, lang.intent2id)

    def __len__(self):
        return len(self.utterances)

    def __getitem__(self, idx):
        utt = torch.Tensor(self.utt_ids[idx])
        
        # Add the CLS token at the beginning of the sequence
        utt = torch.cat([torch.tensor([bert_tokenizer.cls_token_id]), utt])
        slots = torch.Tensor(self.slot_ids[idx])
        intent = self.intent_ids[idx]
        sample = {'utterance': utt, 'slots': slots, 'intent': intent}
        return sample
    
    # Auxiliary methods
    
    def mapping_lab(self, data, mapper):
        return [mapper[x] if x in mapper else mapper[self.unk] for x in data]
    
    def mapping_seq(self, data, mapper): 
        '''
            Map sequences to number
            Args:
                data: list of sequences
                mapper: function to map the token to ids (tokenizer)
            Returns:
                res: list of token ids
        '''
        res = []
        for seq in data:
            tmp_seq = []
            for x in seq.split():
                if x in mapper:
                    tmp_seq.append(mapper[x])
                else:
                    tmp_seq.append(mapper[self.unk])
            res.append(tmp_seq)
        return res
    
    def mapping_bert(self, data, mapper): 
        '''
            Tokenize the sentence with the bert tokenizer
            Args:
                data: list of sentences
                mapper: function to map the token to ids (tokenizer)
            Returns:
                res: list of token ids
        '''
        res = []
        for seq in data:
            tmp_seq = []
            for x in seq.split():
                    tmp_seq.append(mapper(x))
              
            res.append(tmp_seq)
        return res
    
class Tokenizer_id():
    def __init__(self, words, intents, slots, cutoff=0):
        self.slot2id = self.lab2id(slots) # Convert the slots to ids
        self.intent2id = self.lab2id(intents, pad=False) # Convert the intents to ids
        self.id2slot = {v:k for k, v in self.slot2id.items()}
        self.id2intent = {v:k for k, v in self.intent2id.items()}
        
    def word2id(self, words, cutoff=0, unk=True):
        '''
            Convert words to tokens ids using the bert tokenizer
            args: 
                words: list of words
            returns:    
                ids: list of tokens ids
        '''
        ids = bert_tokenizer.tokenize(words)
        # This is foundamental, as we take only the first subword token
        ids = bert_tokenizer.convert_tokens_to_ids(ids)[0]
        return ids
    
    def id2word(self, ids):
        '''
            Convert tokens ids to words using the bert tokenizer
            args: 
                ids: list of token ids
            returns:    
                words: list of words
        '''
        # Convert the ids to integers
        ids = int(ids)
        return bert_tokenizer.convert_ids_to_tokens(ids)
    
    def lab2id(self, elements, pad=True):
        '''
            args: 
                elements: set of elements
            returns:    
                vocab: dictionary with elements as keys and ids as values
        '''
        vocab = {}
        if pad:
            vocab['pad'] = PAD_TOKEN
        for elem in elements:
                vocab[elem] = len(vocab)
        return vocab